{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f566477",
   "metadata": {},
   "source": [
    "## Generative Sequencers with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e109f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import librosa\n",
    "import aubio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76813e21",
   "metadata": {},
   "source": [
    "### Defining and Checking the AE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485adc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(16)),\n",
    "    tf.keras.layers.Dense(8,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(4,activation='relu')\n",
    "    ])\n",
    "\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4)),\n",
    "    tf.keras.layers.Dense(8,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16,activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder,decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f449d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = np.array([[1,0,0,1,1,0,0,0,1,0,1,1,0,1,0,0]])\n",
    "print(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89606838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed forward test vectors through NNs\n",
    "\n",
    "ff = autoencoder.predict(test_vector)\n",
    "print('AE output: ',ff)\n",
    "\n",
    "encoder_ff = encoder.predict(test_vector)\n",
    "print('Encoder output: ',encoder_ff)\n",
    "\n",
    "decoder_ff = decoder.predict(encoder_ff)\n",
    "print('Decoder output: ',decoder_ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a86705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test vector: ', test_vector)\n",
    "#Threshold AE output\n",
    "op = []\n",
    "for i in ff[0]:\n",
    "    if i > 0.5:\n",
    "        op.append(1)\n",
    "    if i < 0.5:\n",
    "        op.append(0)\n",
    "print('AE output: ',op)\n",
    "\n",
    "\n",
    "#Threshold decoder output\n",
    "op=[]\n",
    "for i in decoder_ff[0]:\n",
    "    if i > 0.5:\n",
    "        op.append(1)\n",
    "    if i < 0.5:\n",
    "        op.append(0)\n",
    "\n",
    "print('Decoder output: ',op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c9472",
   "metadata": {},
   "source": [
    "Using the test vector here is only to test that the NN architecture is succcessful and all parts are connected and can be accessed. Stacked AE architecture is deterministic, so feeding values through it will result in the same outputs.  As the weights and biases are initialised to random values the outputs from the test vector will currently be arbitrary but consistant.  Training the AE will eventually result in accurate reconstruction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4264a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(test_vector, test_vector, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f991b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed forward test vectors through NNs\n",
    "\n",
    "ff = autoencoder.predict(test_vector)\n",
    "print('AE output: ',ff)\n",
    "\n",
    "encoder_ff = encoder.predict(test_vector)\n",
    "print('Encoder output: ',encoder_ff)\n",
    "\n",
    "decoder_ff = decoder.predict(encoder_ff)\n",
    "print('Decoder output: ',decoder_ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271fc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test vector: ', test_vector)\n",
    "#Threshold AE output\n",
    "op = []\n",
    "for i in ff[0]:\n",
    "    if i > 0.5:\n",
    "        op.append(1)\n",
    "    if i < 0.5:\n",
    "        op.append(0)\n",
    "print('AE output: ',op)\n",
    "\n",
    "\n",
    "#Threshold decoder output\n",
    "op=[]\n",
    "for i in decoder_ff[0]:\n",
    "    if i > 0.5:\n",
    "        op.append(1)\n",
    "    if i < 0.5:\n",
    "        op.append(0)\n",
    "\n",
    "print('Decoder output: ',op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885246fc",
   "metadata": {},
   "source": [
    "## Encoding Audio Files\n",
    "\n",
    "Here is the main code for the data pipeline.  This encodes the data, firstly as many-hot encoded 16th note step on/off binary values, secondly as 48-PPQN offset values for micro-timing 'groove'.\n",
    "\n",
    "48-PPQN = 48 Pulses per Quarter Note\n",
    "\n",
    "therefore:\n",
    "- 48 divisions between 1/4 notes\n",
    "- 24 divisions between 1/8 notes\n",
    "- 12 divisins between 1/16 notes\n",
    "- 6 divisions between 1/32 notes\n",
    "\n",
    "FYI: MIDI beat clock runs at 24-PPQN however MIDI ticks are 960 ticks per QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = '/usr/home/folder/file.wav'\n",
    "\n",
    "#Setting variables for time-base etc.\n",
    "per_quarter_note = 48\n",
    "timebase= per_quarter_note*4\n",
    "sr = 44100\n",
    "win_s = 512\n",
    "hop_length = win_s // 2\n",
    "dur = None\n",
    "bar_length = 1\n",
    "# Load the audio file\n",
    "signal_full, sr = librosa.load(filename, sr=sr, mono=True, duration=dur)\n",
    "onsets = librosa.onset.onset_detect(y=signal_full, sr=sr, backtrack = False, units='samples')\n",
    "\n",
    "# Set the desired range for onsets\n",
    "desired_range = 16 * bar_length\n",
    "\n",
    "# Check if onsets is greater than the desired range\n",
    "while len(onsets) > desired_range:\n",
    "    # Double the bar_length\n",
    "    bar_length *= 2\n",
    "\n",
    "    # Recalculate the desired range based on the updated bar_length\n",
    "    desired_range = 16 * bar_length\n",
    "\n",
    "\n",
    "#set 16ths and ppqn values based on bar length\n",
    "num_ppqn = timebase*bar_length\n",
    "sixteenths_div = 16*bar_length\n",
    "\n",
    "#Define ppqn and sixteenth note sizes\n",
    "ppqn_timebase = round(signal_full.shape[0] / num_ppqn)\n",
    "sixteenths = round(signal_full.shape[0] / sixteenths_div)\n",
    "\n",
    "#Initialise slice arrays\n",
    "steps = []\n",
    "ppqn_slices = []\n",
    "\n",
    "#Get 16th note slice points\n",
    "for f in range(sixteenths_div):\n",
    "    out= f * sixteenths\n",
    "    steps=np.append(steps, out)\n",
    "#Get PPQN slice points\n",
    "for f in range(num_ppqn):\n",
    "    out= f * ppqn_timebase\n",
    "    ppqn_slices=np.append(ppqn_slices, out)\n",
    "\n",
    "# Convert onset times to integers using np.round\n",
    "onsets = np.round(onsets).astype(int)\n",
    "\n",
    "# Round onset points down to the nearest sixteenth note and disgard double event triggers\n",
    "onset_points_rounded = []\n",
    "previous_rounded_onset = None\n",
    "keep_mask = np.ones(len(onsets), dtype=bool)\n",
    "for i, onset in enumerate(onsets):\n",
    "    rounded_onset = int(round(onset / sixteenths))\n",
    "    # Check if the current rounded onset is the same as the previous one\n",
    "    if rounded_onset != previous_rounded_onset:\n",
    "        onset_points_rounded.append(rounded_onset)\n",
    "        previous_rounded_onset = rounded_onset\n",
    "    else:\n",
    "        # If the onset is a duplicate, mark it for removal\n",
    "        keep_mask[i] = False\n",
    "\n",
    "# Filter the 'onsets' array to keep only the onsets that haven't been processed\n",
    "onsets = onsets[keep_mask]\n",
    "\n",
    "#Round onsets to nearest PPQN timebase point\n",
    "ppqn_onsets = [int(onset // ppqn_timebase) * ppqn_timebase for onset in onsets]\n",
    "\n",
    "# One-hot encode the rounded onset points in an array of length 16\n",
    "num_sixteenths = sixteenths_div\n",
    "ohe_sixteenths = np.zeros(num_sixteenths)\n",
    "for onset in onset_points_rounded:\n",
    "    if onset < num_sixteenths:\n",
    "        ohe_sixteenths[onset] = 1\n",
    "\n",
    "#Get distances of substeps from nearest 16th note\n",
    "substeps = []\n",
    "quantize_points_rounded_sixteenths = [int(round(ppqn_timebase / sixteenths)) * sixteenths for ppqn_timebase in onsets]\n",
    "for f, c in zip(ppqn_onsets, quantize_points_rounded_sixteenths):\n",
    "    m = f - c\n",
    "    ss = m //ppqn_timebase\n",
    "    s = (ss - -6) / (6 - -6)\n",
    "    if s >= 1:\n",
    "        substeps = np.append(substeps,s)\n",
    "    else:\n",
    "        substeps = np.append(substeps,s)\n",
    "\n",
    "\n",
    "#Match substep values to corresponding event\n",
    "substeps_full = []\n",
    "j = 0  # Index for substeps array\n",
    "for binary_value in ohe_sixteenths:\n",
    "    if binary_value == 1 and j < len(substeps):\n",
    "        substeps_full.append(substeps[j])\n",
    "        j += 1\n",
    "    else:\n",
    "        substeps_full.append(0)\n",
    "\n",
    "ss_arr = np.array(substeps_full)\n",
    "\n",
    "\n",
    "# Concatenate the two arrays horizontally\n",
    "#op= np.concatenate((ohe_sixteenths, substeps_full))\n",
    "print(bar_length, ' bars long')\n",
    "\n",
    "if bar_length == 1:\n",
    "    op= np.concatenate((ohe_sixteenths, substeps_full))\n",
    "    print(op)\n",
    "\n",
    "else:\n",
    "    split1 = np.split(ohe_sixteenths,bar_length)\n",
    "    split2 = np.split(ss_arr,bar_length)\n",
    "    stack = np.empty(32)\n",
    "    for i in range(bar_length):\n",
    "        join = np.concatenate((split1[i], split2[i]))\n",
    "        stack = np.vstack((stack,join))\n",
    "    print(split1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43576dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(signal_full,label='Waveform',zorder=0)\n",
    "plt.vlines(onsets, -1, 1, alpha=1, color='r',\n",
    "           linestyle='-', label='onsets', zorder=1)\n",
    "plt.vlines(steps, -1, 1, alpha=1, color='y',\n",
    "               linestyle='--', label='16th', zorder=1)\n",
    "plt.show\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(signal_full,label='Waveform',zorder=0)\n",
    "plt.vlines(onsets, -1, 1, alpha=1, color='r',\n",
    "           linestyle='-', label='onsets', zorder=1)\n",
    "plt.vlines(ppqn_slices, -1, 1, alpha=1, color='b',\n",
    "           linestyle='--', label='ppqn', zorder=1)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51921a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = '/usr/home/folder/file.wav'\n",
    "\n",
    "#Setting variables for time-base etc.\n",
    "per_quarter_note = 48\n",
    "timebase= per_quarter_note*4\n",
    "sr = 44100\n",
    "win_s = 512\n",
    "hop_length = win_s // 2\n",
    "dur = None\n",
    "bar_length = 1\n",
    "# Load the audio file\n",
    "signal_full, sr = librosa.load(filename, sr=sr, mono=True, duration=dur)\n",
    "# Load the audio file\n",
    "s = aubio.source(filename, sr, hop_length)\n",
    "sr = aubio.source.get_samplerate(s)\n",
    "\n",
    "#aubio.onset.set_threshold\n",
    "o = aubio.onset(\"default\", win_s, hop_length, sr)\n",
    "\n",
    "aubio.onset.set_threshold(o,0.8)\n",
    "\n",
    "# list of onsets, in samples\n",
    "onsets = []\n",
    "\n",
    "total_frames = 0\n",
    "while True:\n",
    "    samples, read = s()\n",
    "    if o(samples):\n",
    "\n",
    "        onsets.append(int(o.get_last()))\n",
    "    total_frames += read\n",
    "    if read < hop_length: break\n",
    "\n",
    "# Set the desired range for onsets\n",
    "desired_range = 16 * bar_length\n",
    "\n",
    "# Check if onsets is greater than the desired range\n",
    "while len(onsets) > desired_range:\n",
    "    # Double the bar_length\n",
    "    bar_length *= 2\n",
    "\n",
    "    # Recalculate the desired range based on the updated bar_length\n",
    "    desired_range = 16 * bar_length\n",
    "\n",
    "\n",
    "#set 16ths and ppqn values based on bar length\n",
    "num_ppqn = timebase*bar_length\n",
    "sixteenths_div = 16*bar_length\n",
    "\n",
    "#Define ppqn and sixteenth note sizes\n",
    "ppqn_timebase = round(signal_full.shape[0] / num_ppqn)\n",
    "sixteenths = round(signal_full.shape[0] / sixteenths_div)\n",
    "\n",
    "#Initialise slice arrays\n",
    "steps = []\n",
    "ppqn_slices = []\n",
    "\n",
    "#Get 16th note slice points\n",
    "for f in range(sixteenths_div):\n",
    "    out= f * sixteenths\n",
    "    steps=np.append(steps, out)\n",
    "#Get PPQN slice points\n",
    "for f in range(num_ppqn):\n",
    "    out= f * ppqn_timebase\n",
    "    ppqn_slices=np.append(ppqn_slices, out)\n",
    "\n",
    "# Convert onset times to integers using np.round\n",
    "onsets = np.round(onsets).astype(int)\n",
    "\n",
    "# Round onset points down to the nearest sixteenth note and disgard double event triggers\n",
    "onset_points_rounded = []\n",
    "previous_rounded_onset = None\n",
    "keep_mask = np.ones(len(onsets), dtype=bool)\n",
    "for i, onset in enumerate(onsets):\n",
    "    rounded_onset = int(round(onset / sixteenths))\n",
    "    # Check if the current rounded onset is the same as the previous one\n",
    "    if rounded_onset != previous_rounded_onset:\n",
    "        onset_points_rounded.append(rounded_onset)\n",
    "        previous_rounded_onset = rounded_onset\n",
    "    else:\n",
    "        # If the onset is a duplicate, mark it for removal\n",
    "        keep_mask[i] = False\n",
    "\n",
    "# Filter the 'onsets' array to keep only the onsets that haven't been processed\n",
    "onsets = onsets[keep_mask]\n",
    "\n",
    "#Round onsets to nearest PPQN timebase point\n",
    "ppqn_onsets = [int(onset // ppqn_timebase) * ppqn_timebase for onset in onsets]\n",
    "\n",
    "# One-hot encode the rounded onset points in an array of length 16\n",
    "num_sixteenths = sixteenths_div\n",
    "ohe_sixteenths = np.zeros(num_sixteenths)\n",
    "for onset in onset_points_rounded:\n",
    "    if onset < num_sixteenths:\n",
    "        ohe_sixteenths[onset] = 1\n",
    "\n",
    "#Get distances of substeps from nearest 16th note\n",
    "substeps = []\n",
    "quantize_points_rounded_sixteenths = [int(round(ppqn_timebase / sixteenths)) * sixteenths for ppqn_timebase in onsets]\n",
    "for f, c in zip(ppqn_onsets, quantize_points_rounded_sixteenths):\n",
    "    m = f - c\n",
    "    ss = m //ppqn_timebase\n",
    "    s = (ss - -6) / (6 - -6)\n",
    "    if s >= 1:\n",
    "        substeps = np.append(substeps,s)\n",
    "    else:\n",
    "        substeps = np.append(substeps,s)\n",
    "\n",
    "\n",
    "#Match substep values to corresponding event\n",
    "substeps_full = []\n",
    "j = 0  # Index for substeps array\n",
    "for binary_value in ohe_sixteenths:\n",
    "    if binary_value == 1 and j < len(substeps):\n",
    "        substeps_full.append(substeps[j])\n",
    "        j += 1\n",
    "    else:\n",
    "        substeps_full.append(0)\n",
    "\n",
    "ss_arr = np.array(substeps_full)\n",
    "\n",
    "\n",
    "# Concatenate the two arrays horizontally\n",
    "#op= np.concatenate((ohe_sixteenths, substeps_full))\n",
    "print('Length in bars: ',bar_length)\n",
    "\n",
    "if bar_length == 1:\n",
    "    op= np.concatenate((ohe_sixteenths, substeps_full))\n",
    "    print(op)\n",
    "\n",
    "else:\n",
    "    split1 = np.split(ohe_sixteenths,bar_length)\n",
    "    split2 = np.split(ss_arr,bar_length)\n",
    "    stack = np.empty(32)\n",
    "    for i in range(bar_length):\n",
    "        join = np.concatenate((split1[i], split2[i]))\n",
    "        stack = np.vstack((stack,join))\n",
    "    print(split1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec747bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot figures of waveform and onset detection with 16th divisions and 48-PPQN\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(signal_full,label='Waveform',zorder=0)\n",
    "plt.vlines(onsets, -1, 1, alpha=1, color='r',\n",
    "           linestyle='-', label='onsets', zorder=1)\n",
    "plt.vlines(steps, -1, 1, alpha=1, color='y',\n",
    "               linestyle='--', label='16th', zorder=1)\n",
    "plt.show\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(signal_full,label='Waveform',zorder=0)\n",
    "plt.vlines(onsets, -1, 1, alpha=1, color='r',\n",
    "           linestyle='-', label='onsets', zorder=1)\n",
    "plt.vlines(ppqn_slices, -1, 1, alpha=1, color='b',\n",
    "           linestyle='--', label='ppqn', zorder=1)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176af8c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Encoding the audio corpus runs in a loop to create the dataset array.  In Deep Steps the proccessed dataset is stored as a CSV to avoid repeating the processing step for re-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load dataset from csv file\n",
    "dataset = np.loadtxt('dataset.csv', delimiter=',')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70eb2b",
   "metadata": {},
   "source": [
    "## Re-Defining Autoencoder for step on/off & micro-timing offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(32)),\n",
    "    tf.keras.layers.Dense(16,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(8,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(4,activation='relu')\n",
    "    ])\n",
    "\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4)),\n",
    "    tf.keras.layers.Dense(8,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32,activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder,decoder])\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "# Train the autoencoder model\n",
    "history = autoencoder.fit(dataset, dataset, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1196a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting loss over epoches\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('AE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sending training data through TRAINED encoder to get vectors to visualise latent space\n",
    "latent_vectors = []\n",
    "for testing in dataset:\n",
    "    test_data = testing\n",
    "\n",
    "    test_data = np.reshape(test_data, (1, 32))\n",
    "\n",
    "    encoded_data = encoder.predict(test_data)\n",
    "\n",
    "    latent_vectors.append(encoded_data[0])\n",
    "\n",
    "\n",
    "latent_vectors = np.array(latent_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting 4 dimensions of latent space\n",
    "\n",
    "plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1])\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Latent Vectors Visualization')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(latent_vectors[:, 2], latent_vectors[:, 3])\n",
    "plt.xlabel('Latent Dimension 3')\n",
    "plt.ylabel('Latent Dimension 4')\n",
    "plt.title('Latent Vectors Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d76cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0918eb5",
   "metadata": {},
   "source": [
    "## Testing trained AE reconstructing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4087ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a test vector from dataset\n",
    "test_vector = np.array([dataset[100]])\n",
    "print(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac37801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed test vector through encoder to see compressed representation\n",
    "encoder_test = encoder.predict(test_vector)\n",
    "print(encoder_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed encoded data through decoder to check reconstruction\n",
    "decoder_test = decoder.predict(encoder_test)\n",
    "\n",
    "#Split into step on/off and offsets\n",
    "split = np.split(decoder_test[0],2)\n",
    "\n",
    "steps = split[0]\n",
    "\n",
    "substeps = split[1]\n",
    "\n",
    "output1 = []\n",
    "output2 = []\n",
    "seq=[]\n",
    "tolerance=0.5\n",
    "\n",
    "#Use threshold and scaling to convert generated rhythm to binary and substeps\n",
    "for op in steps:\n",
    "    if op > tolerance:\n",
    "        output1.append(1)\n",
    "    else: output1.append(0)\n",
    "\n",
    "for s in substeps:\n",
    "    op = int(s * (6 - -6) + -6)\n",
    "    output2.append(op)\n",
    "\n",
    "print('Test Vector: ',test_vector)\n",
    "print('AE: Raw Generation ',decoder_test)\n",
    "print('Step on/off: ',output1)\n",
    "print('Substeps raw: ',substeps)\n",
    "print('48-PPQN substeps ',output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5efe3d",
   "metadata": {},
   "source": [
    "## Ex Nihilo Generation\n",
    "\n",
    "Here we use various methods of creating new latent vectors to feed through decoder to generate new parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually creating latent vector\n",
    "\n",
    "a_value = 0.1\n",
    "b_value = 1.5\n",
    "c_value = 2.3\n",
    "d_value = 0.2\n",
    "\n",
    "latent_vector = np.array([[a_value, b_value, c_value, d_value]])\n",
    "\n",
    "generated_rhythm = decoder.predict(latent_vector)\n",
    "split = np.split(generated_rhythm[0],2)\n",
    "\n",
    "steps = split[0]\n",
    "\n",
    "substeps = split[1]\n",
    "\n",
    "output1 = []\n",
    "output2 = []\n",
    "seq=[]\n",
    "tolerance=0.5\n",
    "for op in steps:\n",
    "    if op > tolerance:\n",
    "        output1.append(1)\n",
    "    else: output1.append(0)\n",
    "        \n",
    "for s in substeps:\n",
    "    op = int(s * (6 - -6) + -6)\n",
    "    output2.append(op)\n",
    "    \n",
    "print('AE: Raw Generation ',generated_rhythm)\n",
    "print('Step on/off: ',output1)\n",
    "print('48-PPQN substeps ',output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random latent vector with values between 0-1\n",
    "\n",
    "latent_vector = np.random.rand(1,4)\n",
    "\n",
    "\n",
    "\n",
    "print(latent_vector)\n",
    "\n",
    "generated_rhythm = decoder.predict(latent_vector)\n",
    "split = np.split(generated_rhythm[0],2)\n",
    "\n",
    "steps = split[0]\n",
    "\n",
    "substeps = split[1]\n",
    "\n",
    "output1 = []\n",
    "output2 = []\n",
    "seq=[]\n",
    "tolerance=0.5\n",
    "for op in steps:\n",
    "    if op > tolerance:\n",
    "        output1.append(1)\n",
    "    else: output1.append(0)\n",
    "        \n",
    "for s in substeps:\n",
    "    op = int(s * (6 - -6) + -6)\n",
    "    output2.append(op)\n",
    "    \n",
    "print('AE: Raw Generation ',generated_rhythm)\n",
    "print('Step on/off: ',output1)\n",
    "print('48-PPQN substeps ',output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50211227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random latent vector using a Gaussian distribution with mean 3 and standard deviation 2.5\n",
    "\n",
    "latent_vector = 3 + 2.5 * np.random.randn(1,4)\n",
    "\n",
    "print(latent_vector)\n",
    "\n",
    "generated_rhythm = decoder.predict(latent_vector)\n",
    "split = np.split(generated_rhythm[0],2)\n",
    "\n",
    "steps = split[0]\n",
    "\n",
    "substeps = split[1]\n",
    "\n",
    "output1 = []\n",
    "output2 = []\n",
    "seq=[]\n",
    "tolerance=0.5\n",
    "for op in steps:\n",
    "    if op > tolerance:\n",
    "        output1.append(1)\n",
    "    else: output1.append(0)\n",
    "        \n",
    "for s in substeps:\n",
    "    op = int(s * (6 - -6) + -6)\n",
    "    output2.append(op)\n",
    "    \n",
    "print('AE: Raw Generation ',generated_rhythm)\n",
    "print('Step on/off: ',output1)\n",
    "print('48-PPQN substeps ',output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
